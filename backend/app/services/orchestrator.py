import shutil
from pathlib import Path
from typing import Optional, Sequence
import asyncio
import re

from app.core.config import settings
from app.core.logger import get_logger
from app.services.media.audio_processor import audio_processor
from app.services.media.noise_cleaner import NoiseCleaner
from app.services.media.video_analyzer import video_analyzer

from app.services.ia.transcriber import transcriber
from app.services.ia.vision_client import vision_client
from app.services.ia.manager import ia_manager

from app.services.nlp.text_cleaner import text_cleaner
from app.services.nlp.document_structurer import document_structurer
from app.db.repositories.transcription_repo import TranscriptionRepository
from app.db.repositories.note_repo import NoteRepository
from app.db.repositories.export_repo import ExportRepository
from app.models.transcription import Transcription
from app.db.mongo import get_database
from app.models.note import Note

from app.services.export.pdf import generate_pdf
from app.services.export.docx import generate_docx
from app.services.export.txt import generate_txt

# üîß CORRECTION FACULTATIVE : Validation des formats d'export
VALID_EXPORT_FORMATS = {"pdf", "docx", "txt"}

class Orchestrator:
    def __init__(self):
        self._repo = None       # Pour les transcriptions
        self._note_repo = None  # Pour les notes
        self._export_repo = None
        self._logger = get_logger("orchestrator")
    
    @property
    def repo(self) -> TranscriptionRepository:
        """Propri√©t√© pour le repository des transcriptions"""
        if self._repo is None:
            db = get_database()
            self._repo = TranscriptionRepository(db)
        return self._repo

    @property
    def note_repo(self) -> NoteRepository:
        """Propri√©t√© pour le repository des notes"""
        if self._note_repo is None:
            db = get_database()
            self._note_repo = NoteRepository(db)
        return self._note_repo
    
    @property
    def export_repo(self) -> ExportRepository:
        """Propri√©t√© pour le repository des exports"""
        if self._export_repo is None:
            db = get_database()
            self._export_repo = ExportRepository(db)
        return self._export_repo
    
    async def process_full_media(
        self,
        media_id: str,
        file_path: Path,
        user_id: Optional[str] = None,
        content_type: Optional[str] = None,
        export_formats: Optional[Sequence[str]] = None,
    ) -> bool:
        """
        Pipeline complet:
        Upload ‚Üí Traitement m√©dia ‚Üí Transcription ‚Üí OCR/Vision ‚Üí Structuration NLP ‚Üí Export ‚Üí Sauvegarde DB
        """
        # üîß CORRECTION FACULTATIVE : Validation et normalisation des formats d'export
        if not export_formats:
            export_formats = ["pdf", "docx", "txt"]
            self._logger.warning("‚ö†Ô∏è Aucun format d'export re√ßu. For√ßage par d√©faut : PDF, DOCX, TXT")
        else:
            # Valider et filtrer les formats invalides
            export_formats = [fmt.lower() for fmt in export_formats if fmt.lower() in VALID_EXPORT_FORMATS]
            if not export_formats:
                self._logger.warning("‚ö†Ô∏è Aucun format valide fourni. Utilisation des formats par d√©faut.")
                export_formats = ["pdf", "docx", "txt"]

        temp_files: list[Path] = []
        temp_dirs: list[Path] = []

        try:
            self._logger.info("üöÄ Pipeline d√©marr√©: media_id=%s", media_id)

            # 1) Media ‚Üí audio
            await self.repo.update_status(media_id, "processing_audio")
            extracted_audio_path: Path = await asyncio.to_thread(audio_processor.extract_audio, file_path)
            temp_files.append(extracted_audio_path)

            # 2) Nettoyage audio (noise reduction)
            cleaned_audio_str = await NoiseCleaner.clean_audio(str(extracted_audio_path))
            cleaned_audio_path = Path(cleaned_audio_str)
            temp_files.append(cleaned_audio_path)

            # 3) Vision (si vid√©o): keyframes ‚Üí OCR
            keyframes: list[Path] = []
            keyframes_dir = None  # üîß CORRECTION BLOQUANTE : Initialisation pour √©viter UnboundLocalError
            if file_path.suffix.lower() in {".mp4", ".mov", ".avi", ".mkv", ".webm"}:
                keyframes_dir = settings.FRAMES_DIR / media_id
                temp_dirs.append(keyframes_dir)
                keyframes = await asyncio.to_thread(
                    video_analyzer.extract_keyframes,
                    file_path,
                    keyframes_dir,
                    2,  # interval_seconds
                    None,
                )

            visual_context = await vision_client.get_visual_context(keyframes)

            # 4) STT ‚Üí Transcription par morceaux (Chunking)
            await self.repo.update_status(media_id, "transcribing")

            # On d√©coupe l'audio nettoy√© en segments de 10 min
            chunks = await asyncio.to_thread(audio_processor.split_audio, cleaned_audio_path)
            temp_files.extend(chunks) # On ajoute les chunks pour le nettoyage final
            
            full_raw_text = []
            full_refined_text = []
            all_segments = []
            detected_language = "fr"

            self._logger.info("üì¶ Audio divis√© en %s segments", len(chunks))

            for i, chunk_path in enumerate(chunks):
                self._logger.info("üéôÔ∏è Transcription segment %d/%d...", i + 1, len(chunks))
                
                # Transcription du morceau
                stt_chunk = await transcriber.process_audio_to_text(chunk_path)
                
                # Accumulation des r√©sultats
                if stt_chunk.get("raw_text"):
                    full_raw_text.append(stt_chunk["raw_text"])
                if stt_chunk.get("refined_text"):
                    full_refined_text.append(stt_chunk["refined_text"])
                
                # On r√©cup√®re la langue du premier chunk
                if i == 0:
                    detected_language = stt_chunk.get("language", "fr")
                
                # Optionnel: on agr√®ge les segments temporels si besoin
                all_segments.extend(stt_chunk.get("segments", []))

            # Fusion finale des textes
            raw_transcript = "\n".join(full_raw_text)
            refined_transcript = "\n".join(full_refined_text)

            if not refined_transcript:
                raise ValueError("La transcription a √©chou√© : aucun texte g√©n√©r√©.")
            
            # 5) Type contenu (si auto)
            effective_content_type = content_type or "auto"
            if effective_content_type == "auto":
                effective_content_type = await ia_manager.detect_content_type(refined_transcript)

            await self.repo.update_status(media_id, "generating_notes")

            # 6) G√©n√©ration notes (Markdown)
            generated_notes = await ia_manager.generate_notes(
                transcription=refined_transcript,
                content_type=effective_content_type,
                visual_context=visual_context,
            )
            generated_notes = text_cleaner.clean(generated_notes)

            # --- üîß CORRECTION BLOQUANTE : Sauvegarde permanente des images AVANT nettoyage ---
            if keyframes:
                # On d√©finit un dossier permanent pour les images de cette note
                permanent_img_dir = Path(settings.STORAGE_DIR) / "notes_assets" / media_id
                permanent_img_dir.mkdir(parents=True, exist_ok=True)
                
                saved_paths = []
                for img in keyframes:
                    dest = permanent_img_dir / img.name
                    shutil.copy(img, dest) # On copie l'image vers le stockage permanent
                    saved_paths.append(dest)
                
                # üîß CORRECTION IMPORTANTE : Utiliser les chemins permanents pour l'int√©gration
                generated_notes = self._integrate_real_captures(generated_notes, saved_paths)
                
                # üîß CORRECTION BLOQUANTE : Retirer keyframes_dir de temp_dirs pour √©viter 
                # la suppression pr√©matur√©e des images sources avant la copie compl√®te
                if keyframes_dir and keyframes_dir in temp_dirs:
                    temp_dirs.remove(keyframes_dir)

            # 7) Structuration NLP (pr√©-export)
            structured = document_structurer.structure_for_export(
                generated_notes,
                content_type=effective_content_type,
                metadata={"media_id": media_id, "user_id": user_id},
            )

            # 8) DB: transcription (texte transcrit) + note (texte g√©n√©r√©)
            transcription_obj = Transcription(
                media_id=media_id,
                user_id=user_id,
                text=refined_transcript,           # Texte fusionn√©
                raw_text=raw_transcript,            # Texte brut fusionn√©
                visual_context=visual_context,
                segments=all_segments,
                language=detected_language,
                model="SmartScribe (Whisper Chunked + OCR + LLM)",
            )

            saved_transcription = await self.repo.create(transcription_obj)

            # Cr√©ation de l'objet Note proprement
            new_note = Note(
                user_id=user_id,
                transcription_id=str(saved_transcription.id),
                media_id=media_id,
                title=structured.get("title", "Notes de cours"),
                content=structured.get("raw_content", generated_notes),
                content_type=effective_content_type,
                generation_params={"visual": bool(visual_context)},
                model_used="groq:llama-3.3-70b",
                status="completed"
            )

            # Appel via l'instance du repo (self.note_repo)
            saved_note = await self.note_repo.create(new_note)
            note_id = saved_note.id # R√©cup√©ration de l'ID pour les exports

            # --- üîß CORRECTION IMPORTANTE : Gestion d'erreurs robuste pour les exports ---
            if export_formats:
                final_content = structured.get("raw_content") or generated_notes
                final_title = structured.get("title") or "Notes de cours"
                
                note_data = {
                    "title": final_title, 
                    "content": final_content
                }
                
                export_results = []
                for fmt in export_formats:
                    fmt = fmt.lower()
                    try:
                        self._logger.info(f"‚è≥ G√©n√©ration de l'export {fmt}...")
                        
                        # G√©n√©ration du fichier selon le format
                        if fmt == "pdf":
                            export_path, file_size = await generate_pdf(note_data)
                        elif fmt == "docx":
                            export_path, file_size = await generate_docx(note_data)
                        elif fmt == "txt":
                            export_path, file_size = await generate_txt(note_data)
                        else:
                            self._logger.warning(f"Format {fmt} non support√©, ignor√©")
                            continue
                        
                        # üîß CORRECTION IMPORTANTE : V√©rifier que le fichier existe r√©ellement
                        if not Path(export_path).exists():
                            raise FileNotFoundError(f"Le fichier {export_path} n'a pas √©t√© cr√©√©")
                        
                        # Sauvegarde en base de donn√©es
                        export_id = await self.export_repo.create({
                            "user_id": user_id,
                            "note_id": str(note_id),
                            "format": fmt,
                            "file_path": str(export_path),
                            "file_size": file_size,
                        })
                        
                        export_results.append({"format": fmt, "id": export_id})
                        self._logger.info(f"‚úÖ Export {fmt} cr√©√© : {export_path}")
                        
                    except Exception as e:
                        # üîß CORRECTION IMPORTANTE : Logger l'erreur compl√®te sans bloquer le pipeline
                        self._logger.error(f"‚ùå Erreur lors de l'export {fmt} : {str(e)}", exc_info=True)
                        # Ne pas bloquer le pipeline pour un export rat√©
                        continue
                
                # üîß CORRECTION IMPORTANTE : Log du r√©sultat global
                if export_results:
                    self._logger.info(f"‚úÖ {len(export_results)} export(s) cr√©√©(s) avec succ√®s")
                else:
                    self._logger.warning("‚ö†Ô∏è Aucun export n'a pu √™tre cr√©√©")
                    
            self._logger.info("‚úÖ Pipeline termin√© avec succ√®s: media_id=%s", media_id)
            return True

        except Exception as e:
            self._logger.error("‚ùå √âCHEC CRITIQUE du pipeline media_id=%s: %s", media_id, str(e))
            # Optionnel : loguer la stacktrace compl√®te ici pour le debug
            import traceback
            self._logger.error(traceback.format_exc())
            return False

        finally:
            # Nettoyage syst√©matique des fichiers temporaires (audio d√©coup√©, etc.)
            self._logger.info("üßπ Nettoyage des fichiers temporaires...")
            self._cleanup(temp_files, temp_dirs)

    async def process_document(  
        self,  
        media_id: str,  
        file_path: Path,  
        user_id: Optional[str] = None,  
        content_type: Optional[str] = None,  
        export_formats: Optional[Sequence[str]] = None,  
    ) -> bool:  
        """  
        üÜï Pipeline pour documents textuels (PDF, DOCX, TXT):  
        Upload ‚Üí Extraction texte ‚Üí G√©n√©ration notes ‚Üí Structuration NLP ‚Üí Export ‚Üí Sauvegarde DB  
        """  
        # Validation des formats d'export  
        if not export_formats:  
            export_formats = ["pdf", "docx", "txt"]  
            self._logger.warning("‚ö†Ô∏è Aucun format d'export re√ßu. For√ßage par d√©faut : PDF, DOCX, TXT")  
        else:  
            export_formats = [fmt.lower() for fmt in export_formats if fmt.lower() in VALID_EXPORT_FORMATS]  
            if not export_formats:  
                self._logger.warning("‚ö†Ô∏è Aucun format valide fourni. Utilisation des formats par d√©faut.")  
                export_formats = ["pdf", "docx", "txt"]  
  
        try:  
            self._logger.info("üöÄ Pipeline document d√©marr√©: media_id=%s", media_id)  
  
            # 1) Extraction du texte selon le format  
            await self.repo.update_status(media_id, "extracting_text")  
              
            file_ext = file_path.suffix.lower()  
            if file_ext == ".pdf":  
                from app.services.document.text_extractor import extract_text_from_pdf  
                text_content = await extract_text_from_pdf(file_path)  
            elif file_ext in [".docx", ".doc"]:  
                from app.services.document.text_extractor import extract_text_from_docx  
                text_content = await extract_text_from_docx(file_path)  
            elif file_ext == ".txt":  
                from app.services.document.text_extractor import extract_text_from_txt  
                text_content = await extract_text_from_txt(file_path)  
            else:  
                raise ValueError(f"Format de document non support√©: {file_ext}")  
              
            if not text_content or len(text_content.strip()) < 50:  
                raise ValueError("Le document ne contient pas assez de texte exploitable")  
              
            self._logger.info(f"üìÑ Texte extrait: {len(text_content)} caract√®res")  
  
            # 2) D√©tection du type de contenu  
            effective_content_type = content_type or "auto"  
            if effective_content_type == "auto":  
                effective_content_type = await ia_manager.detect_content_type(text_content)  
                self._logger.info("üîç Type d√©tect√©: %s", effective_content_type)  
              
            # 3) G√©n√©ration de notes via IA  
            await self.repo.update_status(media_id, "generating_notes")  
            generated_notes = await ia_manager.generate_notes(  
                transcript=text_content,  
                content_type=effective_content_type,  
                visual_context=None  # Pas de contexte visuel pour les documents  
            )  
              
            # 4) Nettoyage du texte  
            cleaned_notes = text_cleaner.clean(generated_notes)  
              
            # 5) Structuration pour export  
            structured = document_structurer.structure_for_export(  
                cleaned_notes,  
                content_type=effective_content_type,  
                metadata={"media_id": media_id, "user_id": user_id},  
            )  
              
            # 6) Sauvegarde en base de donn√©es  
            from app.models.transcription import Transcription  
            transcription_obj = Transcription(  
                media_id=media_id,  
                user_id=user_id,  
                text=text_content,  
                raw_text=text_content,  
                visual_context=None,  
                segments=[],  
                language="fr",  
                model_name="Document Extraction",  
            )  
              
            saved_transcription = await self.repo.create(transcription_obj)  
              
            # 7) Cr√©ation de la note  
            from app.models.note import Note  
            new_note = Note(  
                user_id=user_id,  
                transcription_id=str(saved_transcription.id),  
                media_id=media_id,  
                title=structured.get("title", "Notes de document"),  
                content=structured.get("raw_content", cleaned_notes),  
                content_type=effective_content_type,  
                generation_params={"source": "document"},  
                model_used="groq:llama-3.3-70b",  
                status="completed"  
            )  
              
            saved_note = await self.note_repo.create(new_note)  
            note_id = saved_note.id  
              
            # 8) Exports  
            if export_formats:  
                final_content = structured.get("raw_content") or cleaned_notes  
                final_title = structured.get("title") or "Notes de document"  
                  
                note_data = {  
                    "title": final_title,  
                    "content": final_content  
                }  
                  
                export_results = []  
                for fmt in export_formats:  
                    fmt = fmt.lower()  
                    try:  
                        self._logger.info(f"‚è≥ G√©n√©ration de l'export {fmt} pour document...")  
                          
                        if fmt == "pdf":  
                            export_path, file_size = await generate_pdf(note_data)  
                        elif fmt == "docx":  
                            export_path, file_size = await generate_docx(note_data)  
                        elif fmt == "txt":  
                            export_path, file_size = await generate_txt(note_data)  
                        else:  
                            self._logger.warning(f"‚ö†Ô∏è Format {fmt} non support√©, ignor√©")  
                            continue  
                          
                        # V√©rification que le fichier a bien √©t√© cr√©√©  
                        if not Path(export_path).exists():  
                            self._logger.error(f"‚ùå Le fichier d'export {fmt} n'a pas √©t√© cr√©√©")  
                            continue  
                          
                        # Sauvegarde en base de donn√©es  
                        export_id = await self.export_repo.create({  
                            "user_id": user_id,  
                            "note_id": str(note_id),  
                            "format": fmt,  
                            "file_path": str(export_path),  
                            "file_size": file_size,  
                        })  
                          
                        export_results.append({"format": fmt, "id": export_id})  
                        self._logger.info(f"‚úÖ Export {fmt} cr√©√© : {export_path}")  
                          
                    except Exception as e:  
                        self._logger.error(f"‚ùå Erreur lors de l'export {fmt} : {str(e)}", exc_info=True)  
                        continue  
                  
                if export_results:  
                    self._logger.info(f"‚úÖ {len(export_results)} export(s) cr√©√©(s) avec succ√®s")  
                else:  
                    self._logger.warning("‚ö†Ô∏è Aucun export n'a pu √™tre cr√©√©")  
              
            self._logger.info("‚úÖ Pipeline document termin√© avec succ√®s: media_id=%s", media_id)  
            return True  
              
        except Exception as e:  
            self._logger.error("‚ùå √âCHEC CRITIQUE du pipeline document media_id=%s: %s", media_id, str(e))  
            import traceback  
            self._logger.error(traceback.format_exc())  
            return False  
  
    def _cleanup(self, files: list[Path], dirs: list[Path]) -> None:  
        """Nettoyage rigoureux des fichiers et dossiers temporaires."""  
        for f in files:  
            if f and f.exists():  
                try:   
                    f.unlink()  
                except Exception:   
                    pass  
          
        for d in dirs:  
            if d and d.exists():  
                try:   
                    shutil.rmtree(d)  
                except Exception:   
                    pass  
  
    def _integrate_real_captures(self, content: str, keyframes: list[Path]) -> str:  
        """  
        üîß CORRECTION IMPORTANTE : Gestion des chemins d'images pour exports PDF/DOCX  
          
        Scanne le document pour trouver ' –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ' et y injecter   
        les captures d'√©cran r√©elles de la vid√©o.  
          
        Note : Utilise le chemin absolu pour que les g√©n√©rateurs PDF/DOCX   
        puissent localiser et embarquer les images correctement.  
        """  
        # Pattern flexible pour capturer la balise avec espaces possibles  
        tag_pattern = r"\s? –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ \s?"  
          
        # On s√©pare le contenu pour traiter chaque occurrence  
        parts = re.split(tag_pattern, content)  
          
        if len(parts) == 1:  
            return content  # Pas de balise trouv√©e  
          
        new_content = parts[0]  
        for i in range(1, len(parts)):  
            # Si on a encore des images disponibles  
            if i-1 < len(keyframes):  
                img_path = keyframes[i-1]  
                # Utilisation du chemin absolu pour que les g√©n√©rateurs PDF/DOCX  
                # puissent localiser les images sur le disque  
                img_tag = f"\n\n![Illustration technique {i}]({img_path.absolute()})\n\n"  
                new_content += img_tag + parts[i]  
            else:  
                # Plus d'images ? On retire simplement la balise  
                new_content += parts[i]  
                  
        return new_content  
  
orchestrator = Orchestrator()
